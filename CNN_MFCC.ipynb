{"cells":[{"cell_type":"markdown","metadata":{},"source":["#  <center> Speech Emotion Recognition <center>"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","#### First, lets define SER i.e. Speech Emotion Recognition.\n","* Speech Emotion Recognition, abbreviated as SER, is the act of attempting to recognize human emotion and affective states from speech. This is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon that animals like dogs and horses employ to be able to understand human emotion.\n","\n","#### Why we need it?\n","\n","1. Emotion recognition is the part of speech recognition which is gaining more popularity and need for it increases enormously. Although there are methods to recognize emotion using machine learning techniques, this project attempts to use deep learning to recognize the emotions from data.\n","\n","2. SER(Speech Emotion Recognition) is used in call center for classifying calls according to emotions and can be used as the performance parameter for conversational analysis thus identifying the unsatisfied customer, customer satisfaction and so on.. for helping companies improving their services\n","\n","3. It can also be used in-car board system based on information of the mental state of the driver can be provided to the system to initiate his/her safety preventing accidents to happen\n","\n","#### Datasets used in this project\n","\n","* Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import os\n","import sys\n","\n","# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n","import librosa\n","import librosa.display\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","\n","# to play the audio files\n","from IPython.display import Audio\n","\n","import keras\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n","from keras.utils import np_utils, to_categorical\n","from keras.callbacks import ModelCheckpoint\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","\n","* We will use this dataframe to extract features for our model training.\n","* Ths is a collab notebook in which datasets are imported from kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Crema DataSet form kaggle \n"]},{"cell_type":"markdown","metadata":{},"source":["## <center>2. Crema DataFrame</center>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["crema_directory_list = os.listdir(Crema)\n","\n","file_emotion = []\n","file_path = []\n","\n","for file in crema_directory_list:\n","    # storing file paths\n","    file_path.append(Crema + file)\n","    # storing file emotions\n","    part=file.split('_')\n","    if part[2] == 'SAD':\n","        file_emotion.append('sad')\n","    elif part[2] == 'ANG':\n","        file_emotion.append('angry')\n","    elif part[2] == 'DIS':\n","        file_emotion.append('disgust')\n","    elif part[2] == 'FEA':\n","        file_emotion.append('fear')\n","    elif part[2] == 'HAP':\n","        file_emotion.append('happy')\n","    elif part[2] == 'NEU':\n","        file_emotion.append('neutral')\n","    else:\n","        file_emotion.append('Unknown')\n","        \n","# dataframe for emotion of files\n","emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","\n","# dataframe for path of files.\n","path_df = pd.DataFrame(file_path, columns=['Path'])\n","Crema_df = pd.concat([emotion_df, path_df], axis=1)\n","Crema_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# creating Dataframe using all the 4 dataframes we created so far.\n","data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n","data_path.to_csv(\"data_path.csv\",index=False)\n","data_path.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Visualisation and Exploration"]},{"cell_type":"markdown","metadata":{},"source":["First let's plot the count of each emotions in our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.title('Count of Emotions', size=16)\n","sns.countplot(data_path.Emotions)\n","plt.ylabel('Count', size=12)\n","plt.xlabel('Emotions', size=12)\n","sns.despine(top=True, right=True, left=False, bottom=False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can also plot waveplots and spectograms for audio signals\n","\n","* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n","* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. Itâ€™s a representation of frequencies changing with respect to time for given audio/music signals."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_waveplot(data, sr, e):\n","    plt.figure(figsize=(10, 3))\n","    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n","    librosa.display.waveplot(data, sr=sr)\n","    plt.show()\n","\n","def create_spectrogram(data, sr, e):\n","    #converts the data into short term fourier transform\n","    X = librosa.stft(data)\n","    Xdb = librosa.amplitude_to_db(abs(X))\n","    plt.figure(figsize=(12, 3))\n","    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n","    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n","    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n","    plt.colorbar()"]},{"cell_type":"markdown","metadata":{},"source":["![image.png](https://www.kaggleusercontent.com/kf/34958802/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7UdUtjtLnk7VxMmDYPp62A.Hqa6i4XZIQOvE4f341FlEwvJ_-URF-JSZWSI_1WB5TRkCe853gkwU3HfY4kY-VAvfEFDsih6HqU4zuMy2IChZ9PAnGuVTx1fzeF28HFVdV0k2262V7NHOxhcgUl2f68DFDes-1QiUTDIWcP9WnAE_31Po5vjYTor6IL8ymAP9OviBgZ7HKS9Q2-t9rVCfhGgySkBnUTQrOYin66l8r9-9d4MEQGkNpedOR5Znz-eA_ZXDugLhJIzaCdycAAycSyW22gyG_ZGSIZGL-M1FbeZoHBXVfOKthYTE8XPGSN1qUFR1ac9EU2sQD53NuhCgBisG-FJqD-_8644F8Eu7-BzYg4oqsiknzu4O3uy4cxGoqURjW8wbGMRCGE0aUnzRbNLVRfcj9S-910ZVjU5dgDm-BPEqCsg-ZKKj4CKpyKWtadW6VpdSV3KSEcBJGrx_IM2CGKX8VlYsNXzibUHU_P2VIygT7Uk4yg9kzWDLhbyxk7bCsiLWpIWW0j3e5IoqQOQymlESUrIUPGfNBOVcUOteQDloHzZuS_dm2sSU07jeCIIY-prguzDdKxp1Y5irSWJQbgSHObj4qofsAr2v3CrvmcrmMpUPWWQM4urnPrQQbUf8Z5N51KVM07zz0EldtXUEHqugQ1YXB8Fo1lRbx7Qx19sjzNpXFejczcwWGJEhGY.ayE72iBfk94pBVGbEP7YSQ/__results___files/__results___17_0.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["emotion='fear'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"markdown","metadata":{},"source":["![image.png](https://www.kaggleusercontent.com/kf/34958802/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7UdUtjtLnk7VxMmDYPp62A.Hqa6i4XZIQOvE4f341FlEwvJ_-URF-JSZWSI_1WB5TRkCe853gkwU3HfY4kY-VAvfEFDsih6HqU4zuMy2IChZ9PAnGuVTx1fzeF28HFVdV0k2262V7NHOxhcgUl2f68DFDes-1QiUTDIWcP9WnAE_31Po5vjYTor6IL8ymAP9OviBgZ7HKS9Q2-t9rVCfhGgySkBnUTQrOYin66l8r9-9d4MEQGkNpedOR5Znz-eA_ZXDugLhJIzaCdycAAycSyW22gyG_ZGSIZGL-M1FbeZoHBXVfOKthYTE8XPGSN1qUFR1ac9EU2sQD53NuhCgBisG-FJqD-_8644F8Eu7-BzYg4oqsiknzu4O3uy4cxGoqURjW8wbGMRCGE0aUnzRbNLVRfcj9S-910ZVjU5dgDm-BPEqCsg-ZKKj4CKpyKWtadW6VpdSV3KSEcBJGrx_IM2CGKX8VlYsNXzibUHU_P2VIygT7Uk4yg9kzWDLhbyxk7bCsiLWpIWW0j3e5IoqQOQymlESUrIUPGfNBOVcUOteQDloHzZuS_dm2sSU07jeCIIY-prguzDdKxp1Y5irSWJQbgSHObj4qofsAr2v3CrvmcrmMpUPWWQM4urnPrQQbUf8Z5N51KVM07zz0EldtXUEHqugQ1YXB8Fo1lRbx7Qx19sjzNpXFejczcwWGJEhGY.ayE72iBfk94pBVGbEP7YSQ/__results___files/__results___20_0.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["emotion='angry'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["emotion='sad'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["emotion='happy'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n","    data = data + noise_amp*np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data, rate=0.8):\n","    return librosa.effects.time_stretch(data, rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sampling_rate, pitch_factor=0.7):\n","    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n","\n","# taking any example and checking for techniques.\n","path = np.array(data_path.Path)[1]\n","data, sample_rate = librosa.load(path)"]},{"cell_type":"markdown","metadata":{},"source":["#### 1. Simple Audio"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(14,4))\n","librosa.display.waveplot(y=data, sr=sample_rate)\n","Audio(path)"]},{"cell_type":"markdown","metadata":{},"source":["#### 2. Noise Injection"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = noise(data)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"markdown","metadata":{},"source":["We can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted"]},{"cell_type":"markdown","metadata":{},"source":["#### 3. Stretching"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = stretch(data)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4. Shifting"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = shift(data)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"markdown","metadata":{},"source":["#### 5. Pitch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = pitch(data, sample_rate)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Extraction\n","- Extraction of features is a very important part in analyzing and finding relations between different things. As we already know that the data provided of audio cannot be understood by the models directly so we need to convert them into an understandable format for which feature extraction is used.\n","\n","\n","The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\n","\n","![image.png](https://miro.medium.com/max/633/1*7sKM9aECRmuoqTadCYVw9A.jpeg)\n","\n","\n","\n","As stated there with the help of the sample rate and the sample data, one can perform several transformations on it to extract valuable features out of it.\n","1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n","2. Energy : The sum of squares of the signal values, normalized by the respective frame length.\n","3. Entropy of Energy : The entropy of sub-framesâ€™ normalized energies. It can be interpreted as a measure of abrupt changes.\n","4. Spectral Centroid : The center of gravity of the spectrum.\n","5. Spectral Spread : The second central moment of the spectrum.\n","6. Spectral Entropy :  Entropy of the normalized spectral energies for a set of sub-frames.\n","7. Spectral Flux : The squared difference between the normalized magnitudes of the spectra of the two successive frames.\n","8. Spectral Rolloff : The frequency below which 90% of the magnitude distribution of the spectrum is concentrated.\n","9.  MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.\n","10. Chroma Vector : A 12-element representation of the spectral energy where the bins represent the 12 equal-tempered pitch classes of western-type music (semitone spacing).\n","11. Chroma Deviation : The standard deviation of the 12 chroma coefficients.\n","\n","\n"," I am only extracting 5 features:\n","- Zero Crossing Rate\n","- Chroma_stft\n","- MFCC\n","- RMS(root mean square) value\n","- MelSpectogram to train our model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def extract_features(data):\n","    # ZCR\n","    result = np.array([])\n","    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n","    result=np.hstack((result, zcr)) # stacking horizontally\n","\n","    # Chroma_stft\n","    stft = np.abs(librosa.stft(data))\n","    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n","    result = np.hstack((result, chroma_stft)) # stacking horizontally\n","\n","    # MFCC\n","    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n","    result = np.hstack((result, mfcc)) # stacking horizontally\n","\n","    # Root Mean Square Value\n","    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n","    result = np.hstack((result, rms)) # stacking horizontally\n","\n","    # MelSpectogram\n","    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n","    result = np.hstack((result, mel)) # stacking horizontally\n","    \n","    return result\n","\n","def get_features(path):\n","    \n","    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n","    \n","  \n","    res1 = extract_features(data)\n","    result = np.array(res1)\n"," \n","    noise_data = noise(data)\n","    res2 = extract_features(noise_data)\n","    result = np.vstack((result, res2)) \n","    new_data = stretch(data)\n","    data_stretch_pitch = pitch(new_data, sample_rate)\n","    res3 = extract_features(data_stretch_pitch)\n","    result = np.vstack((result, res3)) \n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X, Y = [], []\n","for path, emotion in zip(data_path.Path, data_path.Emotions):\n","    feature = get_features(path)\n","    for ele in feature:\n","        X.append(ele)\n","        Y.append(emotion)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(X), len(Y), data_path.Path.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Features = pd.DataFrame(X)\n","Features['labels'] = Y\n","Features.to_csv('features.csv', index=False)\n","Features.head()"]},{"cell_type":"markdown","metadata":{},"source":["* We have applied data augmentation and extracted the features for each audio files and saved them."]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","\n","- As of now we have extracted the data, now we need to normalize and split our data for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X = Features.iloc[: ,:-1].values\n","Y = Features['labels'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# As this is a multiclass classification problem onehotencoding our Y.\n","encoder = OneHotEncoder()\n","Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# splitting data\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# scaling our data with sklearn's Standard scaler\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# making our data compatible to model.\n","x_train = np.expand_dims(x_train, axis=2)\n","x_test = np.expand_dims(x_test, axis=2)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Modelling"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model=Sequential()\n","model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","\n","model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","\n","model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","\n","model.add(Flatten())\n","model.add(Dense(units=32, activation='relu'))\n","model.add(Dropout(0.3))\n","\n","model.add(Dense(units=8, activation='softmax'))\n","model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n","history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n","\n","epochs = [i for i in range(50)]\n","fig , ax = plt.subplots(1,2)\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","test_acc = history.history['val_accuracy']\n","test_loss = history.history['val_loss']\n","\n","fig.set_size_inches(20,6)\n","ax[0].plot(epochs , train_loss , label = 'Training Loss')\n","ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n","ax[0].set_title('Training & Testing Loss')\n","ax[0].legend()\n","ax[0].set_xlabel(\"Epochs\")\n","\n","ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n","ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n","ax[1].set_title('Training & Testing Accuracy')\n","ax[1].legend()\n","ax[1].set_xlabel(\"Epochs\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# predicting on test data.\n","pred_test = model.predict(x_test)\n","y_pred = encoder.inverse_transform(pred_test)\n","\n","y_test = encoder.inverse_transform(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n","df['Predicted Labels'] = y_pred.flatten()\n","df['Actual Labels'] = y_test.flatten()\n","\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize = (12, 10))\n","cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n","sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n","plt.title('Confusion Matrix', size=20)\n","plt.xlabel('Predicted Labels', size=14)\n","plt.ylabel('Actual Labels', size=14)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["              precision    recall  f1-score   support\n","\n","       angry       0.78      0.69      0.73      1396\n","        calm       0.62      0.86      0.72       142\n","     disgust       0.54      0.48      0.51      1461\n","        fear       0.63      0.51      0.57      1443\n","       happy       0.53      0.62      0.57      1450\n","     neutral       0.55      0.57      0.56      1265\n","         sad       0.58      0.68      0.62      1470\n","    surprise       0.85      0.79      0.82       495\n","\n","    accuracy                           0.61      9122\n","\n"," macro avg        0.63      0.65      0.64      9122\n","\n","weighted avg       0.61      0.61      0.61      9122"]},{"cell_type":"markdown","metadata":{},"source":["- We can see our model is more accurate in predicting surprise, angry emotions and it makes sense also because audio files of these emotions differ to other audio files in a lot of ways like pitch, speed etc..\n","- We overall achieved 61% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods.\n","- The Methods are similar for SVM MODEL We just use the svm model instead of the cnn model"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
